# -*- coding:utf-8 -*-
import sys
import re
import time
import logging
from bs4 import BeautifulSoup

import config
import util


class FundCrawler():
    """
    æŒ‰codeå–åŸºé‡‘åŸºæœ¬ä¿¡æ¯ å­˜å…¥è¡¨ä¸­
    """

    def __init__(self, k, y):
        self.fundcode = k
        self.p_fund = y

    """
    æ•°æ®æ¥å£ æ„é€ url
    """
    # åŸºé‡‘æ¡£æ¡ˆ åŸºæœ¬ä¿¡æ¯
    __interface_basic = 'http://fundf10.eastmoney.com/jbgk_%s.html'
    # åŸºé‡‘ç»ç†
    __interface_manager = 'http://fundf10.eastmoney.com/jjjl_%s.html'
    # èµ„äº§é…ç½®
    __interface_zcpz = 'http://fundf10.eastmoney.com/zcpz_%s.html'
    # æŒæœ‰äººç»“æ„
    __interface_cyrjg = 'http://fundf10.eastmoney.com/FundArchivesDatas.aspx?type=cyrjg&code=%s'
    # è¡Œä¸šé…ç½® http://fundf10.eastmoney.com/hytz_519732.html
    # http://api.fund.eastmoney.com/f10/HYPZ/?fundCode=519732&year=&callback=jQuery18300498687738752952_1620113642720&_=1620113643107

    def crawl_all(self):
        fund_cursor = self.p_fund.find({'code': self.fundcode})
        if len(list(fund_cursor)) > 0:
            logging.info('æ•°æ®åº“ä¸­å·²å­˜åœ¨è¯¥åŸºé‡‘ï¼Œä¸éœ€è¦é‡å¤æŠ“å–ï¼åŸºé‡‘ä»£ç  : %s' % (self.fundcode))
        else:
            """
            æŠ“å–æ‰€æœ‰åŸºé‡‘æ•°æ®
            """
            data_basic = self.crawl_basic()
            data_manager = self.crawl_manager()
            data_zcpz = self.crawl_zcpz()
            data_cyrjg = self.crawl_cyrjg()
            data = {
                'code': self.fundcode,
                'basic': data_basic,
                'managers': data_manager,
                'zcpz': data_zcpz,
                'cyrjg': data_cyrjg,
            }
            logging.info('å³å°†å­˜å…¥%s: ' % (data))

            # # dataå…ƒç»„ä¸ä¸ºç©º ä¿å­˜å½“å‰codeæ‰€æœ‰æ•°æ®
            if data:
                data['createDate'] = util.current
                # ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
                result = self.p_fund.insert_one(data)
                # è®°å½•å…¥åº“å¼€å§‹æ—¶é—´
                logging.info('code %så­˜å…¥æ•°æ®åº“' % (self.fundcode))

    def crawl_cyrjg(self):
        """
        æŒæœ‰äººç»“æ„
        """
        url = self.__interface_cyrjg % (self.fundcode)
        logging.info('åŸºé‡‘ä»£ç %sï¼Œ ' % (self.fundcode))
        # æŠ“å–ç¬¬ä¸€é¡µæ•°æ®
        html = util.fetch_page(logging, url)
        # è§£ææ•°æ®
        phObj = self.parse_html_cyrjg(html)
        data = phObj['obj']
        logging.info('æŒæœ‰äººç»“æ„ %s: ' % (data))

        return data

    def parse_html_cyrjg(self, html):
        """
        è§£æhtml æŒæœ‰äººç»“æ„
        """
        # å°†htmlä¼ å…¥BeautifulSoup çš„æ„é€ æ–¹æ³•,å¾—åˆ°ä¸€ä¸ªæ–‡æ¡£çš„å¯¹è±¡
        soup = BeautifulSoup(html, 'html.parser')
        # åŸºæœ¬æ¦‚å†µ
        # æ ‡é¢˜  tableæ ‡ç­¾ä¸‹trä¸‹thæ ‡ç­¾
        titlelist = soup.select("table > thead th ")
        # logging.info('åŸºæœ¬ä¿¡æ¯ titlelist %s: ' % (titlelist))
        titles = [item.get_text() for item in titlelist]
        logging.info('æŒæœ‰äººç»“æ„ %s: ' % (titles))

        # å†…å®¹  txt_inç±»ä¸‹ tableæ ‡ç­¾ä¸‹trä¸‹tdæ ‡ç­¾
        contentlabellist = soup.select("table > tbody td ")
        logging.info('æŒæœ‰äººç»“æ„ contentlabellist %s: ' % (contentlabellist))

        # ç›®æ ‡æ•°æ®å­˜å‚¨åˆ—è¡¨
        datalist = []
        # å†…å®¹åˆ—è¡¨
        contents = []
        for index, item in enumerate(contentlabellist):

            # æ¯5æ¡zipä¿å­˜
            if index != 0 and (index % 5 == 0 or index == len(contentlabellist)-1):
                # if (index % 5 == 0 or index == len(contentlabellist)-1):
                contents.append(item.get_text())
                data = zip(titles, contents)
                data = dict(data)
                # dataä¸ä¸ºç©º
                datalist.append(data)
                contents = []
            contents.append(item.get_text())

        logging.info('æŒæœ‰äººç»“æ„ %s: ' % (datalist))

        return {'obj': datalist}

    def crawl_zcpz(self):
        """
        èµ„äº§é…ç½®
        """
        url = self.__interface_zcpz % (self.fundcode)
        logging.info('åŸºé‡‘ä»£ç %sï¼Œ ' % (self.fundcode))
        # æŠ“å–ç¬¬ä¸€é¡µæ•°æ®
        html = util.fetch_page(logging, url)
        # è§£ææ•°æ®
        phObj = self.parse_html_zcpz(html)
        data = phObj['obj']
        logging.info('åŸºæœ¬ä¿¡æ¯%s: ' % (data))

        return data

    def parse_html_zcpz(self, html):
        """
        è§£æhtml èµ„äº§é…ç½®
        """
        # å°†htmlä¼ å…¥BeautifulSoup çš„æ„é€ æ–¹æ³•,å¾—åˆ°ä¸€ä¸ªæ–‡æ¡£çš„å¯¹è±¡
        soup = BeautifulSoup(html, 'html.parser')
        # åŸºæœ¬æ¦‚å†µ
        # æ ‡é¢˜  txt_inç±»ä¸‹ tableæ ‡ç­¾ä¸‹trä¸‹thæ ‡ç­¾
        titlelist = soup.select(
            ".box.nb > .boxitem > table > thead > tr > th ")
        # logging.info('åŸºæœ¬ä¿¡æ¯ titlelist %s: ' % (titlelist))
        # bug--ä¼šå¾—åˆ°thä¸‹æ‰€æœ‰å†…å®¹åŒ…æ‹¬å­æ ‡ç­¾æ–‡æœ¬å†…å®¹
        titles = [item.get_text() for item in titlelist]
        logging.info('èµ„äº§é…ç½®%s: ' % (titles))

        # å†…å®¹  txt_inç±»ä¸‹ tableæ ‡ç­¾ä¸‹trä¸‹tdæ ‡ç­¾
        contentlabellist = soup.select(
            ".box.nb > .boxitem > table > tbody td ")
        logging.info('èµ„äº§é…ç½® contentlabellist %s: ' % (contentlabellist))

        # ç›®æ ‡æ•°æ®å­˜å‚¨åˆ—è¡¨
        datalist = []
        # å†…å®¹åˆ—è¡¨
        contents = []
        for index, item in enumerate(contentlabellist):
            # æ¯5æ¡zipä¿å­˜
            if index != 0 and (index % 5 == 0 or index == len(contentlabellist)-1):
                contents.append(item.get_text())
                data = zip(titles, contents)
                data = dict(data)
                # dataä¸ä¸ºç©º
                datalist.append(data)
                contents = []
            contents.append(item.get_text())

        logging.info('èµ„äº§é…ç½® %s: ' % (datalist))

        return {'obj': datalist}

    def crawl_basic(self):
        """
        æŠ“å–åŸºæœ¬æ¡£æ¡ˆæ•°æ®
        """
        url = self.__interface_basic % (self.fundcode)
        logging.info('åŸºé‡‘ä»£ç %sï¼Œ ' % (self.fundcode))

        # æŠ“å–ç¬¬ä¸€é¡µæ•°æ®
        html = util.fetch_page(logging, url)
        logging.info('åŸºæœ¬ä¿¡æ¯html%s: ' % (html))

        # è§£ææ•°æ®
        phObj = self.parse_html_basic(html)
        data = phObj['obj']
        logging.info('åŸºæœ¬ä¿¡æ¯%s: ' % (data))

        return data

    def parse_html_basic(self, html):
        """
        è§£æhtml
        """
        # å°†htmlä¼ å…¥BeautifulSoup çš„æ„é€ æ–¹æ³•,å¾—åˆ°ä¸€ä¸ªæ–‡æ¡£çš„å¯¹è±¡
        soup = BeautifulSoup(html, 'html.parser')

        # åŸºæœ¬æ¦‚å†µ
        # æ ‡é¢˜  txt_inç±»ä¸‹ tableæ ‡ç­¾ä¸‹trä¸‹thæ ‡ç­¾
        titlelist = soup.select(".txt_in table th ")
        titles = [item.get_text() for item in titlelist]

        # å†…å®¹  txt_inç±»ä¸‹ tableæ ‡ç­¾ä¸‹trä¸‹tdæ ‡ç­¾
        contentlist = soup.select(".txt_in table td ")
        contents = [item.get_text() for item in contentlist]
        logging.info('æ¡£æ¡ˆåŸºæœ¬ä¿¡æ¯ contents %s: ' % (contents))

        # æ‰“åŒ…å…ƒç»„
        data = zip(titles, contents)
        data = dict(data)
        data['åŸºé‡‘ä»£ç '] = re.findall('\d+', data['åŸºé‡‘ä»£ç '])[0]

        # ç›˜ä¸­ä¼°ç®—
        # gs = soup.select("#fund_gsz")[0].get_text()
        # print('ä¼°ç®—ï¼š',gs)
        # # ç›˜ä¸­ä¼°ç®—æ¶¨å¹…
        # gszf = soup.select("#fund_gszf")[0].get_text()
        # print('ä¼°ç®—æ¶¨å¹…ï¼š ',gszf)

        return {'obj': data}

    def crawl_manager(self):
        """
        æŠ“å–åŸºé‡‘ç»ç†æ•°æ®
        """
        url = self.__interface_manager % (self.fundcode)
        logging.info('åŸºé‡‘ä»£ç %sï¼Œ ' % (self.fundcode))

        # æŠ“å–ç¬¬ä¸€é¡µæ•°æ®
        html = util.fetch_page(logging, url)
        # è§£ææ•°æ®
        phObj = self.parse_html_manager(html)
        data = phObj['obj']
        logging.info('åŸºé‡‘ç»ç†%s: ' % (data))

        return data

    def parse_html_manager(self, html):
        """
        è§£æhtml åŸºé‡‘ç»ç†
        """
        # å°†htmlä¼ å…¥BeautifulSoup çš„æ„é€ æ–¹æ³•,å¾—åˆ°ä¸€ä¸ªæ–‡æ¡£çš„å¯¹è±¡
        soup = BeautifulSoup(html, 'html.parser')
        # åŸºæœ¬æ¦‚å†µ
        # æ ‡é¢˜  txt_inç±»ä¸‹ tableæ ‡ç­¾ä¸‹trä¸‹thæ ‡ç­¾
        titlelist = soup.select(".txt_in > .box > .boxitem > table th ")
        # logging.info('åŸºæœ¬ä¿¡æ¯ titlelist %s: ' % (titlelist))
        titles = [item.get_text() for item in titlelist]
        # logging.info('åŸºæœ¬ä¿¡æ¯titles %s: ' % (titles))

        # å†…å®¹  txt_inç±»ä¸‹ tableæ ‡ç­¾ä¸‹trä¸‹tdæ ‡ç­¾
        contentlabellist = soup.select(
            ".txt_in > .box > .boxitem > table > tbody td ")
        # logging.info('åŸºæœ¬ä¿¡æ¯contentlabellist %s: ' % (contentlabellist))

        # ç›®æ ‡æ•°æ®å­˜å‚¨åˆ—è¡¨
        datalist = []
        # å†…å®¹åˆ—è¡¨
        contents = []
        for index, item in enumerate(contentlabellist):
            # æ¯5æ¡zipä¿å­˜
            if index != 0 and (index % 5 == 0 or index == len(contentlabellist)-1):
                contents.append(item.get_text())
                data = zip(titles, contents)
                data = dict(data)
                datalist.append(data)
                contents = []

            contents.append(item.get_text())

        logging.info('åŸºé‡‘ç»ç†%s: ' % (datalist))

        return {'obj': datalist}


if __name__ == '__main__':
    # nodejsä¼ è¿›æ¥çš„æ•°ç»„
    fundcodes = sys.argv[1].split(',')

    # è®°å½•æ—¥å¿— -*- coding:utf-8 -*-
    logfilename = '%s/fund_info_%s.log' % (config.logfileDir, fundcodes)
    logging.basicConfig(filename=logfilename,
                        level=logging.DEBUG, format='%(asctime)s %(message)s')
    # æ•°æ®åº“
    db = util.connect_db()
    p_fund = db[config.fund]

    for index, code in enumerate(fundcodes):
        # print('ğŸŠ ', code)
        crawler = FundCrawler(code, p_fund)
        # è®°å½•å¼€å§‹çˆ¬å–æ—¶é—´
        logging.debug('å¼€å§‹çˆ¬å–')
        crawler.crawl_all()
        # è®°å½•çˆ¬å–ç»“æŸæ—¶é—´
        logging.debug('å®Œæˆçˆ¬å–')
        print('åŸºé‡‘è¯¦æƒ…æŠ“å–å®Œæ¯•ï¼')

# æµ‹è¯•å‘½ä»¤  python3 fund2db_basic1code.py
