# -*- coding:utf-8 -*-
import sys
import math
import re
import time
import logging
from bs4 import BeautifulSoup

import config
import util


class FundCrawler():
    """
    æŠ“å–ä¸å­˜åœ¨äºhistoryè¡¨ä¸­çš„å¤šåªåŸºé‡‘å‡€å€¼æ•°æ®ï¼Œä¿å­˜åˆ°mongodbä¸­
    """
    def __init__(self, k, x, y):
        self.fundcode = k
        self.p_daliy = x
        self.p_history = y

    """
    æ•°æ®æ¥å£
    """
    __interface = 'http://eid.csrc.gov.cn/fund/web/list_net_classification.daily_report?1=1&fundCode=%s&classification=%s&limit=20&start=%s'

    def crawl_all(self):
        """
        æŠ“å–æ‰€æœ‰åŸºé‡‘æ•°æ®
        """
        # æŸ¥historyä¸­æ˜¯å¦æœ‰è¯¥code æ²¡æœ‰åˆ™æŠ“å–
        his_cursor = self.p_history.find({'code': self.fundcode})
        if len(list(his_cursor)) > 0:
            logging.info('æ•°æ®åº“ä¸­å·²å­˜åœ¨è¯¥åŸºé‡‘ï¼Œä¸éœ€è¦é‡å¤æŠ“å–ï¼åŸºé‡‘ä»£ç  : %s' % (self.fundcode))
        else:
            param = self.get_interface_param(self.fundcode)
            url = self.__interface % (
                param['fundCode'], param['classification'], 0)
            logging.info('é¦–é¡µurl : %s' % (url))

            data = []
            # æŠ“å–ç¬¬ä¸€é¡µæ•°æ®
            html = util.fetch_page(logging, url)
            # fetch_page è¿”å›æœ‰è¯¯åˆ™å¢åŠ é—´éš”é‡æŸ¥
            fetch_i = 1
            while html == '':
                fetch_i += 1
                if fetch_i > 20:
                    time.sleep(1)
                else:
                    time.sleep(0.5)
                html = util.fetch_page(logging, url)

            # è§£ææ•°æ®
            # æ‰¾åˆ°daliyè¡¨ä¸­è¯¥åŸºé‡‘è¯¦æƒ…
            fundinfo = list(self.p_daliy.find({'code': self.fundcode}))[0]
            logging.info('åŸºé‡‘ä»£ç %s å­˜åœ¨äºdaliyçš„æ•°æ® %s' % (self.fundcode, fundinfo))

            phObj = self.parse_html(html, fundinfo)
            data += phObj['data']
            # æ€»æ¡æ•°
            num = phObj['num']
            # æ€»é¡µæ•°
            page = math.ceil(int(num)/20)
            # è®°å½•
            logging.info('åŸºé‡‘ä»£ç %s æ€»å…±%sæ¡ï¼Œ %sé¡µ' % (self.fundcode, num, page))
            # print('åŸºé‡‘ä»£ç %s æ€»å…±%sæ¡ï¼Œ %sé¡µ' % (fundcode, num, page))
            # ç¬¬numé¡µ

            n = 1
            while n < int(page):
                url = self.__interface % (
                    param['fundCode'], param['classification'], n*20)

                # å¤„ç†ç¬¬né¡µæ•°æ®å¹¶è·å–æ€»é¡µæ•°
                html = util.fetch_page(logging, url)

                # fetch_page è¿”å›æœ‰è¯¯åˆ™å¢åŠ é—´éš”é‡æŸ¥
                fetch_ii = 1
                while html == '':
                    fetch_ii += 1
                    if fetch_ii > 20:
                        time.sleep(5)
                    else:
                        time.sleep(2)
                    html = util.fetch_page(logging, url)
                phObj = self.parse_html(html, fundinfo)

                data += phObj['data']
                time.sleep(0.5)  # 0.5ç§’æŠ“ä¸€é¡µ
                if n % 10 == 0:  # æ¯10é¡µæ˜¾ç¤ºä¸€ä¸ªè¿›åº¦
                    print('[ç¬¬%s/%sé¡µ]' % (n, page))
                else:
                    print('.', end=' ', flush=True)
                n += 1
            # ä¿å­˜å½“å‰codeæ‰€æœ‰æ•°æ®
            # logging.info('å†å²å‡€å€¼æ•°æ® %s: ' % (data))
            if len(data) > 0:
                # ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
                result = p_history.insert_many(data)
                # è®°å½•å…¥åº“å¼€å§‹æ—¶é—´
                logging.info('code%så­˜å…¥æ•°æ®åº“ ï¼Œå…±%sæ¡' % (self.fundcode, len(data)))
            else:
                logging.info('0æ¡å†å²å‡€å€¼æ•°æ®,%sæ•°æ®åº“æ²¡æœ‰æ–°å¢æ•°æ®', config.history)

    def get_interface_param(self, code):
        # æ‰¾åˆ°å½“å‰codeå¯¹åº”çš„idStr
        fund_ori = list(self.p_daliy.find({'code': code}))
        idStr = fund_ori[0]['idStr']
        classification = fund_ori[0]['classification']
        # print(idStr,classification)
        fund_target = list(self.p_daliy.aggregate([
            {"$match": {"idStr": idStr}},
            {"$group": {"_id": {"idStr": "$idStr"},
                        "code": {"$push": "$code"},
                        "classification": {"$push": "$classification"},
                        "type": {"$push": "$type.code"},
                        "shortName": {"$push": "$shortName"}
                        }
             }
        ]))
        fundCode = fund_target[0]['code'][0]
        return {'idStr': idStr, 'classification': classification, 'fundCode': fundCode}

    def parse_html(self, html, fund):
        """
        è§£æhtml
        """
        # logging.warning('html %s: ' % (html))
        # å°†htmlä¼ å…¥BeautifulSoup çš„æ„é€ æ–¹æ³•,å¾—åˆ°ä¸€ä¸ªæ–‡æ¡£çš„å¯¹è±¡
        soup = BeautifulSoup(html, 'html.parser')
        # è§£ææ€»æ¡æ•°
        numTd = soup.select("#con_one_1 tr > td ")[1]
        numstr = re.findall('\d+', numTd.get_text())[2]
        # è§£ææ•°æ®
        tds = soup.select(".cc ~ tr > td ")  # ccç±»æ‰€æœ‰å…„å¼ŸèŠ‚ç‚¹ä¸‹çš„æ‰€æœ‰td
        # logging.warning('htmlçš„td %s: ' % (tds))

        datalist = []  # æ–‡æ¡£ç›®æ ‡æ•°æ®å­˜å‚¨åˆ—è¡¨
        # æ¯æ¡æ•°æ®åˆ—è¡¨
        dtemp = []

        # [è‚¡ç¥¨å‹ æ··åˆå‹ å€ºåˆ¸å‹ QDIIå‹] PK [è´§å¸å‹ çŸ­æœŸç†è´¢å€ºåˆ¸å‹] è¿”å›çš„å­—æ®µä¸åŒ
        ftype = fund['type']
        idStr = fund['idStr']

        if ftype['name'] in config.fundType1:
            head = config.head1
        elif ftype['name'] in config.fundType2:
            head = config.head2
        else:
            logging.warning('æœªå®šä¹‰çš„ç±»å‹ %s' % (fund))
            return

        # å°†åˆ—è¡¨è½¬å­—å…¸
        for key in tds:
            # logging.warning('%s tdsçš„key %s' % (key, key.get_text()))
            # logging.info(key.get_text())

            # if key.get_text() == ' \n':
            if key.get_text() == ' ':
                data = zip(head, dtemp)
                ddict = dict(data)

                # è¿‡æ»¤æ— æ•ˆæ•°æ®
                if self.is_valid_data(ftype['name'], ddict):
                    # æ·»åŠ åˆ›å»ºæ—¥æœŸ
                    ddict['createDate'] = util.current
                    # æ·»åŠ typeç±»å‹ idStrå°ç±»
                    ddict['type'] = ftype
                    ddict['idStr'] = idStr
                    datalist.append(ddict)

                dtemp = []
                continue
            else:
                dtemp.append(key.get_text())  # ä¿å­˜æ•°æ®åˆ°åˆ—è¡¨ä¸­

        # logging.warning('å»subcodeå‰datalist  %s' % (datalist))

        # å»subcode
        datalist = self.column_clean(datalist)
        return {'num': numstr, 'data': datalist}

    def column_clean(self, datalist):
        """ 
        åˆå¹¶code&subcode 
        """
        targetlist = []

        for item in datalist:
            funddict = {
                'createDate': '',
                'zcNetValue': '',
                'valuationDate': '',
                'type': {'code': '', 'name': ''},
                'idStr': '',
                'shortName': '',
                'code': '',
                'shareNetValue': '', 'totalNetValue': '',
                'gainPer': '', 'yearSevenDayYieldRate': '', 'yearSevenDayYieldRatePercent': ''
            }
            if item['subcode'] != '-':
                funddict['code'] = item['subcode']
            else:
                funddict['code'] = item['code']

            funddict['createDate'] = item['createDate']
            funddict['zcNetValue'] = item['zcNetValue']
            funddict['valuationDate'] = item['valuationDate']
            funddict['type']['code'] = item['type']['code']
            funddict['type']['name'] = item['type']['name']
            funddict['idStr'] = item['idStr']
            funddict['shortName'] = item['shortName']

            funddict['shareNetValue'] = item['shareNetValue']
            funddict['totalNetValue'] = item['totalNetValue']

            if 'gainPer' in item:
                funddict['gainPer'] = item['gainPer']
            if 'yearSevenDayYieldRatePercent' in item:
                funddict['yearSevenDayYieldRatePercent'] = item['yearSevenDayYieldRatePercent']
            if 'yearSevenDayYieldRatePercent' in item:
                funddict['yearSevenDayYieldRate'] = float(
                    item['yearSevenDayYieldRatePercent'].strip('%'))/100
            targetlist.append(funddict)  # æˆ–targetlist += [funddict]
        return targetlist

    def is_valid_data(self, typename, htmldatadict):
        is_valid = False
        if typename in config.fundType1 and self.equal_type1_filter_condition(htmldatadict):
            is_valid = True
        if typename in config.fundType2 and self.equal_type2_filter_condition(htmldatadict):
            is_valid = True
        return is_valid

    def equal_type1_filter_condition(self, htmldatadict):
        return (htmldatadict['shareNetValue'] != '' or htmldatadict['zcNetValue'] != '')

    def equal_type2_filter_condition(self, htmldatadict):
        return (htmldatadict['shareNetValue'] != '' or htmldatadict['gainPer'] != '' or htmldatadict['yearSevenDayYieldRate'] != '' or htmldatadict['zcNetValue'] != '')


if __name__ == '__main__':

    date = util.yesterday()

    db = util.connect_db()
    # æŒ‡å®šæ“ä½œçš„é›†åˆ type:collection  daliy
    p_daliy = db[config.daliy]
    p_history = db[config.history]

    # nodejsä¼ è¿›æ¥çš„æ•°ç»„
    fundcodes = sys.argv[1].split(',')  # å¤šåªåŸº

    print('ğŸ˜„', fundcodes)

    # è®°å½•æ—¥å¿— -*- coding:utf-8 -*-
    logfilename = '%s/fundhistory_%s.log' % (config.logfileDir, fundcodes)
    logging.basicConfig(filename=logfilename,
                        level=logging.DEBUG, format='%(asctime)s %(message)s')
    for index, code in enumerate(fundcodes):
        crawler = FundCrawler(code, p_daliy, p_history)
        # è®°å½•å¼€å§‹çˆ¬å–æ—¶é—´
        logging.info('å¼€å§‹çˆ¬å–%s', code)
        crawler.crawl_all()
        # è®°å½•çˆ¬å–ç»“æŸæ—¶é—´
        logging.info('å®Œæˆçˆ¬å–%s', code)
    print('history ok')
